{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 65,
>>>>>>> 1067506264f1593a5949ca0698348d12fdfe50c7
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaneeza Kamil\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (27,30,34,35,36,41,42,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.polynomial.polynomial as poly\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(\"qudditch_training_0_1.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101268, 48)\n",
      "(101267, 20)\n",
      "(101268, 1)\n",
      "['Gryffindor' 'Female' 11.0 6.0 25.0 1.0 1.0 '38' 41.0 0.0 1.0 0.0 0.0 0.0\n",
      " 'Beater' 1.0 'None' '0' '0' 0.0]\n",
      "['Gryffindor' 'Female' 11.0 6.0 25.0 1.0 1.0 '38' 41.0 0.0 1.0 0.0 0.0 0.0\n",
      " 'Beater' 1.0 'None' '0' 0 0]\n",
      "[11.0 6.0 25.0 1.0 1.0 '38' 41.0 0.0 1.0 0.0 0.0 0.0 'Beater' 1.0 'None'\n",
      " '0' 0 0 1.0 0.0 0.0 0.0 1.0 0.0]\n",
      "[[1.0 0.0 0.0 0.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 1.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 1.0]\n",
      " [0.0 0.0 0.0 1.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 1.0]\n",
      " [0.0 0.0 0.0 1.0]\n",
      " [0.0 0.0 0.0 1.0]\n",
      " [1.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 1.0]\n",
      " [0.0 0.0 0.0 0.0]]\n",
      "[[1.0 0.0]\n",
      " [1.0 0.0]\n",
      " [1.0 0.0]\n",
      " [0.0 1.0]\n",
      " [0.0 1.0]\n",
      " [0.0 1.0]\n",
      " [0.0 1.0]\n",
      " [0.0 1.0]\n",
      " [1.0 0.0]\n",
      " [1.0 0.0]\n",
      " [1.0 0.0]\n",
      " [0.0 1.0]\n",
      " [1.0 0.0]\n",
      " [0.0 1.0]\n",
      " [1.0 0.0]\n",
      " [0.0 1.0]\n",
      " [0.0 1.0]\n",
      " [1.0 0.0]\n",
      " [0.0 1.0]\n",
      " [0.0 1.0]]\n"
     ]
    }
   ],
   "source": [
    "#data = (df[~np.isnan(df).any(axis=1)])\n",
    "\n",
    "print(df.shape)\n",
    "data = df.values\n",
    "X0 = data[0:101267,2:47]\n",
    "X1 = np.delete(X0,[3,8,18,21,22,23,24,25,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43],1)\n",
    "#X1:house, gender, age, foul_type_id, game_move_id,penalty_id,game_duration,move_speciality,num_game_moves,\n",
    "#   num_game_losses,num_practice_sessions,num_games_satout,num_games_injured,num_games_notpartof,player_type,\n",
    "#   num_games_won,stooging?,body_blow?,power_play?,sloth_grip_roll?,change,snitch_caught\n",
    "\n",
    "Y0 = data[:,-1]\n",
    "print(X1.shape)\n",
    "\n",
    "#Y0 = np.array(Y0)\n",
    "Y = Y0=='YES'\n",
    "Y = Y.reshape((Y.shape[0],1))\n",
    "print(Y.shape)\n",
    "\n",
    "gryffindor = np.zeros((X1.shape[0],1))\n",
    "hufflepuff = np.zeros((X1.shape[0],1))\n",
    "ravenclaw = np.zeros((X1.shape[0],1))\n",
    "slytherin = np.zeros((X1.shape[0],1))\n",
    "\n",
    "female = np.zeros((X1.shape[0],1))\n",
    "male = np.zeros((X1.shape[0],1))\n",
    "\n",
    "print(X1[0])\n",
    "\n",
    "\"\"\"\n",
    "if you want, we can use this to get rid of the Yes/No in the csv and turn them into 1/0:\n",
    "I feel like 1/0 \n",
    "\"\"\"\n",
    "for row in data:\n",
    "    for column in row:\n",
    "        if column == \"Yes\" or \"YES\":\n",
    "            column = 1\n",
    "        elif column == \"No\" or \"NO\":\n",
    "            column = 0\n",
    "\n",
    "for row in range(len(X1)):\n",
    "    if X1[row][0]==\"Gryffindor\":\n",
    "        gryffindor[row]=True\n",
    "    elif X1[row][0]==\"Hufflepuff\":\n",
    "        hufflepuff[row]=True\n",
    "    elif X1[row][0]==\"Ravenclaw\":\n",
    "        ravenclaw[row]=True\n",
    "    elif X1[row][0]==\"Slytherin\":\n",
    "        slytherin[row]=True\n",
    "        \n",
    "    if X1[row][1]==\"Female\":\n",
    "        female[row]=True\n",
    "    elif X1[row][1]==\"Male\":\n",
    "        male[row]=True\n",
    "        \n",
    "    if X1[row][-1]==\"Yes\":\n",
    "        X1[row][-1]=1\n",
    "    else:\n",
    "        X1[row][-1]=0\n",
    "        \n",
    "    if X1[row][-2]==\"Ch\":\n",
    "        X1[row][-2]=1\n",
    "    else:\n",
    "        X1[row][-2]=0\n",
    "        \n",
    "    if X1[row][7]==\"?\":\n",
    "        X1[row][7]=0;\n",
    "    else:\n",
    "        X1[row][7]=float(X1[row][7])\n",
    "        \n",
    "        \n",
    "#print(X1[0])\n",
    "\n",
    "X1 = np.hstack((X1,gryffindor,hufflepuff,ravenclaw,slytherin,female,male))\n",
    "X = np.delete(X1,[0,1],1)\n",
    "#print(X[0])\n",
    "\n",
    "#print(X[0:20,-6:-2])\n",
    "#print(X[0:20,-2:])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 66,
>>>>>>> 1067506264f1593a5949ca0698348d12fdfe50c7
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)\n",
    "\n",
    "trainX,testX,trainY,testY=train_test_split(X,Y,test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Steady'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d282b9f61334>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#train test split:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#preprocessing to scale the data and use train_test_split to split the data (70% train and 30% test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mx_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Vaneeza Kamil\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mscale\u001b[1;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[0;32m    131\u001b[0m     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\n\u001b[0;32m    132\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'the scale function'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                     dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Vaneeza Kamil\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                       force_all_finite)\n\u001b[0;32m    401\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Steady'"
     ]
    }
   ],
   "source": [
    "\"\"\"this is from the slides for logisitic regression:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver=‘lbfgs',multi_class='multinomial')\n",
    "clf.fit(X_train, y_train)\n",
    "yhat_test = clf.predict(X_test)\n",
    "score = clf.score(X_test, y_test) \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "#helpful link for log reg: https://towardsdatascience.com/logistic-regression-a-simplified-approach-using-python-c4bc81a87c31\n",
    "\n",
    "#train test split:\n",
    "#preprocessing to scale the data and use train_test_split to split the data (70% train and 30% test)\n",
    "x_scale = preprocessing.scale(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scale, Y)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\"\"\"\n",
    "#this trains and predicts\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "predictions = logmodel.predict(X_test)\n",
    "\n",
    "#check precision, recall, f1-score using classification report\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"another option found in hw5\"\"\"\n",
    "#log reg with lasso(l1) regularization\n",
    "# Initialize an empty list to store values of training set accuracy.\n",
    "acc_train_logreg = [] \n",
    "# Initialize an empty list to store values of test set accuracy.\n",
    "acc_test_logreg = []\n",
    "# Initialize an empty list to store different values of parameter 'c'.\n",
    "c_logreg = []\n",
    "# Complete the function logreg that accepts 'c' as a parameter, which is used to create logreg model \n",
    "# with different values of C.\n",
    "def logreg_model(c , X_train, Y_train, X_test, Y_test):\n",
    "    # Create an object of logistic regression model using linear_model.\n",
    "    # Pass the value of penalty as 'L1'. By default, it is 'L2'.\n",
    "    # Pass the value of C = c. Note that C is the inverse of lambda. So, small value of C i.e. b/w 0 and 1 \n",
    "    # means stronger regularization and large value means less regularization.\n",
    "    # Also, in sklearn, L1 is only supported with solver = 'saga'. Solver is the type of optimization algorithm like GDA or\n",
    "    # SGDA, which is to be used. So, 'saga' is another algorithm like that. Pass the value of solver as 'saga'\n",
    "\n",
    "    #Create the Logistic Regression model object as described above and save it to logreg\n",
    "    logreg = linear_model.LogisticRegression(C=c,penalty='l1', warm_start=True, solver='saga')\n",
    "    \n",
    "    #Fit the model on the training set\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    \n",
    "    #Find the prediction on training set\n",
    "    Yhat_train = logreg.predict(X_train)\n",
    "    \n",
    "    # Adding training accuracy to acc_train_logreg\n",
    "    acc_train = np.mean(Yhat_train == Y_train)\n",
    "    acc_train_logreg.append(acc_train)\n",
    "    print(\"Accuracy on training data = %f\" % acc_train)\n",
    "    \n",
    "    #Find the prediction on test set\n",
    "    Yhat_test = logreg.predict(X_test)\n",
    "    \n",
    "    # Adding testing accuracy to acc_test_logreg\n",
    "    acc_test = np.mean(Yhat_test == Y_test)\n",
    "    acc_test_logreg.append(acc_test)\n",
    "    print(\"Accuracy on test data = %f\" % acc_test)\n",
    "    \n",
    "    # Appending value of c for graphing purposes\n",
    "    c_logreg.append(c)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#pick an appropriate value for c and CHANGE the next line:\n",
    "c = 10000\n",
    "logreg_model(c,x_train,y_train,x_test,x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def f_deriv(z):\n",
    "    return f(z) * (1 - f(z))\n",
    "\n",
    "def f2(z):\n",
    "    if z>0:\n",
    "        return z\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def f2_deriv(z):\n",
    "    if z>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def f3(z):\n",
    "    return (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "\n",
    "def f3_deriv(z):\n",
    "    return 1-(f3(z))^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} #creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) #Return “continuous uniform” random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b\n",
    "\n",
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b\n",
    "\n",
    "def feed_forward(x, W, b, act_func=1):\n",
    "    a = {1: x}\n",
    "    z = {}\n",
    "    for l in range(1, len(W) + 1):\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]\n",
    "        if act_func==1:\n",
    "            a[l+1] = f(z[l+1])\n",
    "        elif act_func == 2:\n",
    "            a[l+1] = f2(z[l+1])\n",
    "        elif act_func == 3:\n",
    "            a[l+1] = f3(z[l+1])\n",
    "        \n",
    "    return a, z\n",
    "\n",
    "def calculate_out_layer_delta(y, a_out, z_out, W, lmda, act_func=1):\n",
    "    if act_func==1:\n",
    "        return -(y-a_out) * f_deriv(z_out) + (lmda/2) * W\n",
    "    elif act_func==2:\n",
    "        return -(y-a_out) * f2_deriv(z_out) + (lmda/2) * W\n",
    "    elif act_func==3:\n",
    "        return -(y-a_out) * f3_deriv(z_out) + (lmda/2) * W\n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l, act_func=1):\n",
    "    if act_func==1:\n",
    "        return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)\n",
    "    elif act_func==2:\n",
    "        return np.dot(np.transpose(w_l), delta_plus_1) * f2_deriv(z_l)\n",
    "    elif act_func==3:\n",
    "        return np.dot(np.transpose(w_l), delta_plus_1) * f3_deriv(z_l)\n",
    "\n",
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25, act_func=1):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            a, z = feed_forward(X[i, :], W, b,act_func)\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l], W[l], lmbda.act_func)\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l],act_func)\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_structure1 = [, 10, 1]\n",
    "nn_structure2 = [, 20, 1]\n",
    "nn_structure3 = [, 5, 1]\n",
    "\n",
    "nn_structure4 = [, 10, 10, 1]\n",
    "nn_structure5 = [, 10, 10, 10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_1, b_1, avg_cost_func_1 = train_nn(nn_structure1, X_train, y_v_train, 3000,act_func=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_2, b_2, avg_cost_func_2 = train_nn(nn_structure1, X_train, y_v_train, 3000,act_func=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_3, b_3, avg_cost_func_3 = train_nn(nn_structure1, X_train, y_v_train, 3000,act_func=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_4, b_4, avg_cost_func_4 = train_nn(nn_structure2, X_train, y_v_train, 3000,act_func=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_5, b_5, avg_cost_func_5 = train_nn(nn_structure3, X_train, y_v_train, 3000,act_func=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_6, b_6, avg_cost_func_6 = train_nn(nn_structure4, X_train, y_v_train, 3000,act_func=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_7, b_7, avg_cost_func_7 = train_nn(nn_structure5, X_train, y_v_train, 3000,act_func=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
