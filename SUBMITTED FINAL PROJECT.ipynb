{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "Code modified from https://github.com/adventuresinML/adventures-in-ml-code/blob/master/neural_network_tutorial.py\n",
    "\n",
    "The notation in this website is almost the same as the notation we are using in class.  Instead of $a$ the author uses $h$, and instead of $N$, the author uses $m$. (I have modified the code below to use $a$ and $N$.)\n",
    "\n",
    "Please read about this implementation starting at page 27 from the website listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first thing we will do is import all the libraries\n",
    "\n",
    "We will be using the lower resolution MINST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits # The MNIST data set is in scikit learn data set\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import numpy as np\n",
    "import numpy.random as r # We will randomly initialize our weights\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "\n",
    "#import gnumpy as gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data\n",
    "\n",
    "After we load the data, we print the shape of the data and a pixelated digit.\n",
    "\n",
    "We also show what the features of one example looks like.\n",
    "\n",
    "The neural net will learn to estimate which digit these pixels represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the quidditch dataset:\n",
      "(101268, 48)\n",
      "['NO']\n"
     ]
    }
   ],
   "source": [
    "db = pd.read_csv(\"qudditch_training.csv\")#load_digits()#\n",
    "#print(digits);\n",
    "X = db.drop(['id','player_id','quidditch_league_player','finbourgh_flick', 'double_eight_loop', 'snitchnip', ],axis=1)\n",
    "#print(X)\n",
    "print(\"The shape of the quidditch dataset:\")\n",
    "print(db.shape)\n",
    "#print(db.dtypes)\n",
    "#plt.gray()\n",
    "#plt.matshow(db['quidditch_league_player'])\n",
    "#plt.show()\n",
    "y = np.array(db['quidditch_league_player'])#.drop(0,axis=0))\n",
    "\n",
    "X['move_speciality'] = np.nan_to_num(pd.to_numeric(X['move_speciality'], errors='coerce'))\n",
    "        \n",
    "print(y[0:1])\n",
    "#print(X[0,:])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Scale the dataset\n",
    "The training features range from 0 to 15.  To help the algorithm converge, we will scale the data to have a mean of 0 and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_x_to_vect(y):\n",
    "    #print(y)\n",
    "    y_vect = np.zeros((len(y), np.amax(y)+1))\n",
    "    #y_vect = np.zeros(y.shape(), 1)\n",
    "    for i in range(len(y)):\n",
    "        #rint(y[i])\n",
    "        y_vect[i, y[i]] = 1\n",
    "    \n",
    "    #print(y_vect)\n",
    "    return y_vect.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house\n",
      "['house_?', 'house_Gryffindor', 'house_Hufflepuff', 'house_Other', 'house_Ravenclaw', 'house_Slytherin']\n",
      "gender\n",
      "['gender_Female', 'gender_Male', 'gender_Unknown/Invalid']\n",
      "age\n",
      "weight\n",
      "['weight_0.968597555', 'weight_98086', 'weight_>200', 'weight_?', 'weight_[0-25)', 'weight_[100-125)', 'weight_[125-150)', 'weight_[150-175)', 'weight_[175-200)', 'weight_[25-50)', 'weight_[50-75)', 'weight_[75-100)']\n",
      "foul_type_id\n",
      "game_move_id\n",
      "penalty_id\n",
      "game_duration\n",
      "player_code\n",
      "['player_code_0.395651058', 'player_code_40066', 'player_code_?', 'player_code_BC', 'player_code_CH', 'player_code_CM', 'player_code_CP', 'player_code_DM', 'player_code_FR', 'player_code_HM', 'player_code_MC', 'player_code_MD', 'player_code_MP', 'player_code_OG', 'player_code_OT', 'player_code_PO', 'player_code_SI', 'player_code_SP', 'player_code_UN', 'player_code_WC']\n",
      "move_speciality\n",
      "num_game_moves\n",
      "num_game_losses\n",
      "num_practice_sessions\n",
      "num_games_satout\n",
      "num_games_injured\n",
      "num_games_notpartof\n",
      "player_type\n",
      "['player_type_?', 'player_type_Beater', 'player_type_Chaser', 'player_type_Keeper', 'player_type_Seeker']\n",
      "num_games_won\n",
      "stooging\n",
      "['stooging_0.832717793', 'stooging_84326', 'stooging_>7', 'stooging_>8', 'stooging_None', 'stooging_Norm']\n",
      "body_blow\n",
      "['body_blow_0.803507594', 'body_blow_81368', 'body_blow_Down', 'body_blow_No', 'body_blow_Steady', 'body_blow_Up']\n",
      "checking\n",
      "['checking_0.984881401', 'checking_99735', 'checking_Down', 'checking_No', 'checking_Steady', 'checking_Up']\n",
      "dopplebeater_defence\n",
      "['dopplebeater_defence_0.993077637', 'dopplebeater_defence_100565', 'dopplebeater_defence_Down', 'dopplebeater_defence_No', 'dopplebeater_defence_Steady', 'dopplebeater_defence_Up']\n",
      "hawkshead_attacking_formation\n",
      "['hawkshead_attacking_formation_0.999150751', 'hawkshead_attacking_formation_101180', 'hawkshead_attacking_formation_Down', 'hawkshead_attacking_formation_No', 'hawkshead_attacking_formation_Steady', 'hawkshead_attacking_formation_Up']\n",
      "no_hands_tackle\n",
      "['no_hands_tackle_0.948995714', 'no_hands_tackle_96101', 'no_hands_tackle_Down', 'no_hands_tackle_No', 'no_hands_tackle_Steady', 'no_hands_tackle_Up']\n",
      "power_play\n",
      "['power_play_0.999990125', 'power_play_101265', 'power_play_No', 'power_play_Steady']\n",
      "sloth_grip_roll\n",
      "['sloth_grip_roll_0.875348093', 'sloth_grip_roll_88643', 'sloth_grip_roll_Down', 'sloth_grip_roll_No', 'sloth_grip_roll_Steady', 'sloth_grip_roll_Up']\n",
      "spiral_dive\n",
      "['spiral_dive_0.895186933', 'spiral_dive_90652', 'spiral_dive_Down', 'spiral_dive_No', 'spiral_dive_Steady', 'spiral_dive_Up']\n",
      "starfish_and_stick\n",
      "['starfish_and_stick_0.999772875', 'starfish_and_stick_101243', 'starfish_and_stick_No', 'starfish_and_stick_Steady']\n",
      "twirl\n",
      "['twirl_0.927991626', 'twirl_93974', 'twirl_Down', 'twirl_No', 'twirl_Steady', 'twirl_Up']\n",
      "wronski_feint\n",
      "['wronski_feint_0.937432109', 'wronski_feint_94930', 'wronski_feint_Down', 'wronski_feint_No', 'wronski_feint_Steady', 'wronski_feint_Up']\n",
      "zig-zag\n",
      "['zig-zag_0.99696838', 'zig-zag_100959', 'zig-zag_Down', 'zig-zag_No', 'zig-zag_Steady', 'zig-zag_Up']\n",
      "bludger_backbeat\n",
      "['bludger_backbeat_0.999624751', 'bludger_backbeat_101228', 'bludger_backbeat_Down', 'bludger_backbeat_No', 'bludger_backbeat_Steady', 'bludger_backbeat_Up']\n",
      "chelmondiston_charge\n",
      "['chelmondiston_charge_0.999970375', 'chelmondiston_charge_101263', 'chelmondiston_charge_No', 'chelmondiston_charge_Steady']\n",
      "dionysus_dive\n",
      "['dionysus_dive_0.999614876', 'dionysus_dive_101227', 'dionysus_dive_No', 'dionysus_dive_Steady', 'dionysus_dive_Up']\n",
      "reverse_pass\n",
      "['reverse_pass_0.465605435', 'reverse_pass_47150', 'reverse_pass_Down', 'reverse_pass_No', 'reverse_pass_Steady', 'reverse_pass_Up']\n",
      "parkins_pincer\n",
      "['parkins_pincer_0.993067762', 'parkins_pincer_100564', 'parkins_pincer_Down', 'parkins_pincer_No', 'parkins_pincer_Steady', 'parkins_pincer_Up']\n",
      "plumpton_pass\n",
      "['plumpton_pass_0.999871625', 'plumpton_pass_101253', 'plumpton_pass_No', 'plumpton_pass_Steady']\n",
      "porskoff_ploy\n",
      "['porskoff_ploy_0.999990125', 'porskoff_ploy_101265', 'porskoff_ploy_No', 'porskoff_ploy_Steady']\n",
      "transylvanian_tackle\n",
      "['transylvanian_tackle_0.99998025', 'transylvanian_tackle_101264', 'transylvanian_tackle_No', 'transylvanian_tackle_Steady']\n",
      "woollongong_shimmy\n",
      "['woollongong_shimmy_0.999990125', 'woollongong_shimmy_101265', 'woollongong_shimmy_No', 'woollongong_shimmy_Steady']\n",
      "change\n",
      "['change_0.537929809', 'change_54474', 'change_Ch', 'change_No']\n",
      "snitch_caught\n",
      "['snitch_caught_0.229879723', 'snitch_caught_23279', 'snitch_caught_No', 'snitch_caught_Yes']\n"
     ]
    }
   ],
   "source": [
    "for index in X.columns:\n",
    "    #print(index)\n",
    "    if (X[index].dtype != object):\n",
    "        #print(X[index])\n",
    "        print(X[index].name)\n",
    "        #print(X[index])\n",
    "        \n",
    "    elif(X[index].dtype == object):\n",
    "        \n",
    "        newCols = [X[index].name + '_' + s for s in X[index].astype('category').cat.categories.tolist()]\n",
    "        \n",
    "        print(X[index].name)\n",
    "        print(newCols)\n",
    "        \n",
    "        X[index] = X[index].astype('category').cat.codes\n",
    "        temp = convert_x_to_vect(X[index])\n",
    "        P = pd.DataFrame(temp, columns = newCols)#, columns = X[index].astype('category').cat.categories.tolist())\n",
    "        X = X.drop([index],axis=1)\n",
    "        X = pd.concat([P, X], axis=1)\n",
    "\n",
    "        #print(X.columns[index].get_loc())\n",
    "        # X = np.delete(X,)\n",
    "        #np.delete(X[index], range(0,len(X)), 1) \n",
    "        #np.insert(X,index,convert_x_to_vect(X[index]))\n",
    "        #print(X)\n",
    "        #break\n",
    "        #print(convert_x_to_vect(X[index]))\n",
    "        #X[index] = X_scale.fit_transform(X[index])\n",
    "        \n",
    "        #print(X[index].astype('category').cat.categories.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d72b3174dc09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_scale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Rafi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Rafi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Rafi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \"\"\"\n\u001b[0;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m--> 612\u001b[1;33m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Rafi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    420\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    421\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Rafi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     41\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     42\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 43\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "X_scale = StandardScaler()\n",
    "\n",
    "X = X_scale.fit_transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.83032810e+00, -1.83032810e+00, -9.26810981e-01,  9.26810981e-01,\n",
       "        3.14246404e-03, -3.14246404e-03,  4.44413721e-03, -4.44413721e-03,\n",
       "        3.14246404e-03, -3.14246404e-03,  1.13309866e-02, -1.13309866e-02,\n",
       "       -7.69762347e-03,  8.35501601e-02, -8.27675829e-02, -8.31442467e-03,\n",
       "       -3.69241321e-01,  1.07132686e+00, -6.59616845e-01, -3.53769380e-01,\n",
       "        1.96283648e-02, -1.93749892e-02, -3.14246404e-03,  5.44296113e-03,\n",
       "       -5.44296113e-03, -7.02690199e-03,  1.93749892e-02, -1.74990916e-02,\n",
       "       -4.44413721e-03, -5.44296113e-03,  5.51437971e-02, -5.39601547e-02,\n",
       "       -9.93778545e-03, -2.91542510e-02,  2.58348440e-01, -2.52580102e-01,\n",
       "       -4.18441435e-02, -3.38646079e-02,  2.78560462e-01, -2.71309362e-01,\n",
       "       -4.80226737e-02,  1.50723654e-02, -1.50723654e-02, -7.48377124e-02,\n",
       "        3.42177025e-01, -3.16850832e-01, -8.98513586e-02, -7.43021016e-02,\n",
       "        3.77362785e-01, -3.54422794e-01, -8.73034395e-02,  3.14246404e-03,\n",
       "       -3.14246404e-03, -4.38112289e-02,  2.31830840e-01, -2.19283567e-01,\n",
       "       -5.67424233e-02, -3.14246404e-03,  2.91542510e-02, -2.79415945e-02,\n",
       "       -7.69762347e-03, -1.04228888e-02,  8.34902151e-02, -8.13650928e-02,\n",
       "       -1.53966155e-02, -2.10848708e-02,  1.23897858e-01, -1.17367596e-01,\n",
       "       -3.29761932e-02, -7.51705741e-02,  4.94513204e-01, -4.69134297e-01,\n",
       "       -1.02948430e-01, -1.97183584e-01, -2.96438405e-01,  4.48204232e-01,\n",
       "       -2.27182080e-01, -1.44020016e-02,  9.34589499e-01, -2.91123104e-01,\n",
       "       -5.67961463e-01, -4.10130132e-01,  1.23591248e+00, -2.18986595e-01,\n",
       "       -3.79977514e-02, -1.39240657e-01, -1.59749870e-01, -7.38303252e-02,\n",
       "       -3.14246404e-03, -2.56122932e-01, -6.84109747e-01, -1.89516019e-01,\n",
       "       -2.77640486e-02, -1.01070664e-01, -3.04813192e-02, -7.65531437e-02,\n",
       "       -2.33113533e-02, -2.27518369e-01, -1.56965023e-01, -3.65363142e-02,\n",
       "       -5.44296113e-03,  1.80057016e-01, -2.17766837e-02, -7.85506346e-02,\n",
       "       -3.77362222e-02, -1.85941898e-02, -1.04228888e-02, -3.08041863e-02,\n",
       "       -9.43761965e-02, -1.15274623e-01,  9.27050343e-01, -9.26995101e-01,\n",
       "       -5.44296113e-03, -1.51188271e-01,  5.80793899e-01, -1.43025473e-01,\n",
       "       -1.22576540e-01, -7.95624481e-02, -4.82354929e-01, -5.29587240e+00,\n",
       "        2.75078093e+00,  4.03125773e+00, -1.16998295e+00, -1.13788830e+00,\n",
       "        1.50067169e+00, -1.06497859e-01, -7.85275191e-01, -1.84843399e+00,\n",
       "       -2.91354888e-01, -2.13616423e-01, -5.03227104e-01, -3.32095971e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = X.values\n",
    "X[0,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Creating training and test datasets\n",
    "We split the data into training and test data sets. We will train the neural network with the training dataset, and evaluate our neural network with the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split the data into training and test set.  60% training and %40 test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Setting up the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "Our target is an integer in the range [0,..,9], so we will have 10 output neuron's in our network.  \n",
    "\n",
    "-  If  $y=0$, we want the output neurons to have the values $(1,0,0,0,0,0,0,0,0,0)$\n",
    "\n",
    "-  If  $y=1$ we want the output neurons to have the values $(0,1,0,0,0,0,0,0,0,0)$\n",
    "-  etc\n",
    "\n",
    "Thus we need to change our target so it is the same as our hoped for output of the neural network.  \n",
    "-  If $y=0$ we change it into the vector $(1,0,0,0,0,0,0,0,0,0)$. \n",
    "-  If $y=1$ we change it into the vector $(0,1,0,0,0,0,0,0,0,0)$\n",
    "-  etc\n",
    "\n",
    "See page 29 from the website listed above\n",
    "\n",
    "The code to covert the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def convert_y_to_vect(y):\n",
    "#    y_vect = np.zeros((len(y), 10))\n",
    "#    for i in range(len(y)):\n",
    "#        y_vect[i, y[i]] = 1\n",
    "#    return y_vect\n",
    "\n",
    "def convert_y_to_vect(y):\n",
    "    #print(y)\n",
    "    y_vect = np.zeros((len(y), 1))\n",
    "    #y_vect = np.zeros(y.shape(), 1)\n",
    "    for i in range(len(y)):\n",
    "        #rint(y[i])\n",
    "        y_vect[i, y[i]==\"YES\"] = 1\n",
    "    \n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the training and test targets to vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# convert digits to vectors\n",
    "#print(y_train.shape)\n",
    "y_v_train = convert_y_to_vect(y_train)\n",
    "#print(y_v_train)\n",
    "y_v_test = convert_y_to_vect(y_test)\n",
    "\n",
    "y_test = convert_y_to_vect(y_test).flatten()\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check to see that our code performs as we expect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YES' 'NO' 'NO' 'YES']\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:4])\n",
    "\n",
    "print(y_v_train[0:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Creating the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The activation function and its derivative\n",
    "\n",
    "We will use the sigmoid activation function:  $f(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "The deriviative of the sigmoid function is: $f'(z) = f(z)(1-f(z))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def f_deriv(z):\n",
    "    return f(z) * (1 - f(z))\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "def dReLU(x):\n",
    "    return 1 * (x > 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def dtanh(x):\n",
    "    return 1 - x * x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and initialing W and b\n",
    "We want the weights in W to be different so that during back propagation the nodes on a level will have different gradients and thus have different update values.\n",
    "\n",
    "We want the  weights to be small values, since the sigmoid is almost \"flat\" for large inputs.\n",
    "\n",
    "Next is the code that assigns each weight a number uniformly drawn from $[0.0, 1.0)$.  The code assumes that the number of neurons in each level is in the python list *nn_structure*.\n",
    "\n",
    "In the code, the weights, $W^{(\\ell)}$ and $b^{(\\ell)}$ are held in a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} #creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) #Return “continuous uniform” random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing $\\triangledown W$ and $\\triangledown b$\n",
    "Creating $\\triangledown W^{(\\ell)}$ and $\\triangledown b^{(\\ell)}$ to have the same size as $W^{(\\ell)}$ and $b^{(\\ell)}$, and setting $\\triangledown W^{(\\ell)}$, and  $\\triangledown b^{(\\ell)}$ to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "Perform a forward pass throught the network.  The function returns the values of $a$ and $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "        a[l+1] = f(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "        \n",
    "        #a[l+1] = ReLU(z[l+1])\n",
    "        #a[l+1] = tanh(z[l+1])) \n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\delta$\n",
    "The code below compute $\\delta^{(s_l)}$ in a function called \"calculate_out_layer_delta\",  and  computes $\\delta^{(\\ell)}$ for the hidden layers in the function called \"calculate_hidden_delta\".  \n",
    "\n",
    "If we wanted to have a different cost function, we would change the \"calculate_out_layer_delta\" function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-a_out) * f_deriv(z_out) \n",
    "    #return -(y-a_out) * dReLU(z_out) \n",
    "    #return -(y-a_out) * dtanh(z_out) \n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l) \n",
    "    #return np.dot(np.transpose(w_l), delta_plus_1) * dReLU(z_l) \n",
    "    #return np.dot(np.transpose(w_l), delta_plus_1) * dtanh(z_l) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Back Propagation Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%10 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            #print(\"below..:\")\n",
    "            #print(X[i, :])\n",
    "            a, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "        \n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the neural network\n",
    "\n",
    "Our code assumes the size of each layer in our network is held in a list.  The input layer will have 64 neurons (one for each pixel in our 8 by 8 pixelated digit).  Our hidden layer has 30 neurons (you can change this value).  The output layer has 10 neurons.\n",
    "\n",
    "Next we create the python list to hold the number of neurons for each level and then run the neural network code with our training data.\n",
    "\n",
    "This code will take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132, 33, 1]\n",
      "Starting gradient descent for 3000 iterations\n",
      "Iteration 0 of 3000\n",
      "Iteration 10 of 3000\n",
      "Iteration 20 of 3000\n",
      "Iteration 30 of 3000\n",
      "Iteration 40 of 3000\n",
      "Iteration 50 of 3000\n",
      "Iteration 60 of 3000\n",
      "Iteration 70 of 3000\n",
      "Iteration 80 of 3000\n",
      "Iteration 90 of 3000\n",
      "Iteration 100 of 3000\n",
      "Iteration 110 of 3000\n",
      "Iteration 120 of 3000\n",
      "Iteration 130 of 3000\n",
      "Iteration 140 of 3000\n",
      "Iteration 150 of 3000\n",
      "Iteration 160 of 3000\n",
      "Iteration 170 of 3000\n",
      "Iteration 180 of 3000\n",
      "Iteration 190 of 3000\n",
      "Iteration 200 of 3000\n",
      "Iteration 210 of 3000\n",
      "Iteration 220 of 3000\n",
      "Iteration 230 of 3000\n",
      "Iteration 240 of 3000\n",
      "Iteration 250 of 3000\n",
      "Iteration 260 of 3000\n",
      "Iteration 270 of 3000\n",
      "Iteration 280 of 3000\n",
      "Iteration 290 of 3000\n",
      "Iteration 300 of 3000\n",
      "Iteration 310 of 3000\n",
      "Iteration 320 of 3000\n",
      "Iteration 330 of 3000\n",
      "Iteration 340 of 3000\n",
      "Iteration 350 of 3000\n",
      "Iteration 360 of 3000\n",
      "Iteration 370 of 3000\n",
      "Iteration 380 of 3000\n",
      "Iteration 390 of 3000\n",
      "Iteration 400 of 3000\n",
      "Iteration 410 of 3000\n",
      "Iteration 420 of 3000\n",
      "Iteration 430 of 3000\n",
      "Iteration 440 of 3000\n",
      "Iteration 450 of 3000\n",
      "Iteration 460 of 3000\n",
      "Iteration 470 of 3000\n",
      "Iteration 480 of 3000\n",
      "Iteration 490 of 3000\n",
      "Iteration 500 of 3000\n",
      "Iteration 510 of 3000\n",
      "Iteration 520 of 3000\n",
      "Iteration 530 of 3000\n",
      "Iteration 540 of 3000\n",
      "Iteration 550 of 3000\n",
      "Iteration 560 of 3000\n",
      "Iteration 570 of 3000\n",
      "Iteration 580 of 3000\n",
      "Iteration 590 of 3000\n",
      "Iteration 600 of 3000\n",
      "Iteration 610 of 3000\n",
      "Iteration 620 of 3000\n",
      "Iteration 630 of 3000\n",
      "Iteration 640 of 3000\n",
      "Iteration 650 of 3000\n",
      "Iteration 660 of 3000\n"
     ]
    }
   ],
   "source": [
    "#nn_structure = [64, 30, 10]\n",
    "nn_structure = [X_train.shape[1], (X_train.shape[1])>>2, 1] \n",
    "print(nn_structure)\n",
    "# train the NN\n",
    "#print(X_train[0])\n",
    "#print(y_v_train)\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 3000)\n",
    "#W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 4000) \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the avg_cost_func\n",
    "#print(avg_cost_func)\n",
    "plt.plot(avg_cost_func)\n",
    "plt.ylabel('Average J')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Assessing accuracy\n",
    "Next we determine what percentage the neural network correctly predicted the handwritten digit correctly on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the prediction accuracy and print\n",
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "\n",
    "print(len(y_test))\n",
    "print(len(y_pred))\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Prediction accuracy is 53.84995186017232%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
